ALG_NAME: "pqn"
TOTAL_TIMESTEPS: 1e7 # with 5e7 for 200M 4 frame skip correpsnds to 200M frames
TOTAL_TIMESTEPS_DECAY: 1e7 # will 5e7 for 200M 4 be used for decay functions (epsilon and lr)
NUM_ENVS: 128 # parallel environments
NUM_STEPS: 32 # steps per environment in each update
EPS_START: 1.
EPS_FINISH: 0.001
EPS_DECAY: 0.1 # ratio of total updates
NUM_EPOCHS: 2 # number of epochs per update
NUM_MINIBATCHES: 32 # minibatches per epoch
NETWORK_NAME: "impala"
NETWORK_WIDTH: 3
NORM_TYPE: "layer_norm" # layer_norm or batch_norm
LR: 0.00025
MAX_GRAD_NORM: 10
LR_LINEAR_DECAY: False
GAMMA: 0.99
LAMBDA: 0.65

# env specific, see https://envpool.readthedocs.io/en/latest/env/atari.html
ENV_NAME: "Pong-v5"
ENV_KWARGS: 
  episodic_life: True # lost life -> done, increases sample efficiency, may hurt in some games
  reward_clip: True # reward into -1, 1
  repeat_action_probability: 0. # sticky actions
  frame_skip: 4
  noop_max: 30

# add jaxpruner config here
# TODO: add start and end ratios instead of steps
PRUNER_KWARGS: 
  pruner: "magnitude"
  sparsity: 0.95
  update_frequency: 300
  start_step: 35000 # 157500 
  end_step: 125000  # for 200M 624000 
  sparsity_distribution: "erk"
  drop_fraction: 0.3

ANALYSIS_KWARGS:
  ANALYSIS_BATCH_PERIOD: 10
  ANALYSIS_SAMPLE_SIZE: 512
  srank_tau: 0.01
  srank_sample_size: 512
  taus: [0.1, 0.01, 0.001, 0.0001, 0]
  grama_taus: [0.1, 0.05, 0.001]

# evaluation
TEST_DURING_TRAINING: True 
TEST_ENVS: 8
EPS_TEST: 0. # 0 for greedy test policy
